\documentclass[11pt,british,a4paper]{article}
%\pdfobjcompresslevel=0
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage[includeheadfoot,margin=0.8 in]{geometry}
\usepackage{siunitx,physics,cancel,upgreek,varioref,listings,booktabs,pdfpages,ifthen,polynom,todonotes}
%\usepackage{minted}
\usepackage[backend=biber]{biblatex}
\DefineBibliographyStrings{english}{%
      bibliography = {References},
}
\addbibresource{sources.bib}
\usepackage{mathtools,upgreek,bigints}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tocloft}
\usepackage[T1]{fontenc}
%\usepackage{fouriernc}
% \usepackage[T1]{fontenc}
\usepackage{mathpazo}
% \usepackage{inconsolata}
%\usepackage{eulervm}
%\usepackage{cmbright}
%\usepackage{fontspec}
%\usepackage{unicode-math}
%\setmainfont{Tex Gyre Pagella}
%\setmathfont{Tex Gyre Pagella Math}
%\setmonofont{Tex Gyre Cursor}
%\renewcommand*\ttdefault{txtt}
\graphicspath{{figs/}}
\usepackage[scaled]{beramono}
\usepackage{fancyhdr}
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{lastpage}
\usepackage{microtype}
\usepackage[font=normalsize]{subcaption}
\usepackage{luacode}
\usepackage[linktoc=all, bookmarks=true, pdfauthor={Anders Johansson},pdftitle={FYS-STK4155 Project 2}]{hyperref}
\usepackage{tikz,pgfplots,pgfplotstable}
\usepgfplotslibrary{colorbrewer}
\usepgfplotslibrary{external}
\tikzset{external/system call={lualatex \tikzexternalcheckshellescape -halt-on-error -interaction=batchmode -jobname "\image" "\texsource"}}
\tikzexternalize%[prefix=tmp/, mode=list and make]
\pgfplotsset{cycle list/Dark2}
\pgfplotsset{compat=1.8}
\pgfmathsetseed{12}
\renewcommand{\CancelColor}{\color{red}}
\let\oldexp=\exp
\renewcommand{\exp}[1]{\mathrm{e}^{#1}}


\labelformat{section}{#1}
\labelformat{subsection}{section~#1}
\labelformat{subsubsection}{paragraph~#1}
\labelformat{equation}{equation~(#1)}
\labelformat{figure}{figure~#1}
\labelformat{table}{table~#1}

\renewcommand{\footrulewidth}{\headrulewidth}

%\setcounter{secnumdepth}{4}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}

\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}
\lstset{rangeprefix=!/,
    rangesuffix=/!,
    includerangemarker=false}
\lstset{showstringspaces=false,
    basicstyle=\small\ttfamily,
    keywordstyle=\color{bluekeywords},
    commentstyle=\color{greencomments},
    numberstyle=\color{bluekeywords},
    stringstyle=\color{redstrings},
    breaklines=true,
    %texcl=true,
    language=Fortran,
    morekeywords={norm2,class,deferred}
}
\colorlet{DarkGrey}{white!20!black}
\newcommand{\eqtag}[1]{\refstepcounter{equation}\tag{\theequation}\label{#1}}
\hypersetup{hidelinks=True}

\sisetup{detect-all}
\sisetup{exponent-product = \cdot, output-product = \cdot,per-mode=symbol}
% \sisetup{output-decimal-marker={,}}
\sisetup{round-mode = off, round-precision=3}
\sisetup{number-unit-product = \ }

\allowdisplaybreaks[4]
\fancyhf{}

\rhead{Project 2}
\rfoot{Page~\thepage{} of~\pageref{LastPage}}
\lhead{FYS-STK4155}

%\definecolor{gronn}{rgb}{0.29, 0.33, 0.13}
\definecolor{gronn}{rgb}{0, 0.5, 0}

\newcommand{\husk}[2]{\tikz[baseline,remember picture,inner sep=0pt,outer sep=0pt]{\node[anchor=base] (#1) {\(#2\)};}}
\newcommand{\artanh}[1]{\operatorname{artanh}{\qty(#1)}}
\newcommand{\matrise}[1]{\begin{pmatrix}#1\end{pmatrix}}
\DeclareMathOperator{\Proj}{Proj}
\DeclareMathOperator{\Col}{Col}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\MSE}{MSE}

\newread\infile

\setcounter{tocdepth}{2}
\numberwithin{equation}{section}

\pgfplotstableset{
    every head row/.style={before row=\toprule,after row=\midrule},
    every last row/.style={after row=\bottomrule},
    columns/Method/.style={string type, column type=l}
}

\pgfplotsset{
    tick align=outside,
    tick pos=left,
    xmajorgrids,
    x grid style={white},
    ymajorgrids,
    y grid style={white},
    axis line style={white},
    axis background/.style={fill=white!89.803921568627459!black}
}

%start
\begin{document}
\tikzexternaldisable
\title{FYS-STK4155: Project 2}
\author{Anders Johansson}
%\maketitle

\begin{titlepage}
%\includegraphics[width=\textwidth]{fysisk.pdf}
\vspace*{\fill}
\begin{center}
\textsf{
    \Huge \textbf{Project 2}\\\vspace{0.5cm}
    \Large \textbf{FYS-STK4155 --- Applied data analysis and machine learning}\\
    \vspace{8cm}
    Anders Johansson\\
    \today\\
}
\vspace{1.5cm}
\includegraphics{uio.pdf}\\
\vspace*{\fill}
\end{center}
\end{titlepage}
\null
\pagestyle{empty}
\newpage

\pagestyle{fancy}
\setcounter{page}{1}

\begin{abstract}
    This project applies several statistical learning methods to data from the famous Ising model.
    Linear regression is used to determine the coupling, while classification of spin configurations as ordered or disordered is achieved with both logistic regression and multi-layered neural networks.
    Finally, neural networks are trained to predict the energy of a spin configuration.

    LASSO regression is found to perfectly reproduce the Ising model and give an \(R^2\) score of \(1\) for the correct choice of regularisation.
    Using \(\num{400}\) training samples requires \(\lambda\in\qty[10^{-3},10^{-2}]\), while \(\num{1000}\) allows for smaller values of \(\lambda\).
    The largest viable \(\lambda\) is \(10^{-1}\) in both cases, after which the performance decreases rapidly.
    Visualisation of the coupling matrices shows LASSO being able to break the symmetry and return the correct model, while Ridge returns a symmetric coupling matrix.
    A neural network with no hidden layers taking in couplings, in practice a linear regressor with stochastic batch gradient descent, also returns a symmetric coupling matrix.
    With a learning rate of \(\num{0.04}\), the neural network converges to give an \(R^2\) score of \(1\) after \(10\) epochs.
    On a \(4\)-core processor a batch size of \(12\) is found to minimise the training time.
    Additionally, a neural network with only one hidden layer is able to predict energies with an \(R^2\) score of more than \(\num{0.98}\) when only fed the spins and not the couplings.


\end{abstract}

\tableofcontents

\section{Introduction}
The Ising model is famous in many fields of science.
Physicists study the model because it represents a magnetic lattice and can model ferromagnetic materials, and also because it has a large university class and has the same behaviour as many other models.
Social scientists like the model because it is a simple correlated system, which can approximate correlated quantities such as dialects.
The Metropolis algorithm, acknowledged as one of the premiere algorithms in computational science, is a method for finding the properties of the Ising model numerically.

This project does not deal with the simulation of a spin lattice, but rather the classification and analysis of pre-calculated spin configurations with corresponding energies.
Thus the Ising model can be used to verify various numerical methods for both regression and classification, and also measure their performance to infer their relative strengths and weaknesses.

The main goal is to compare simple neural networks with the different linear regression methods for regression and logistic regression for classification.
The regression part of the problem is to find the coupling constants from given spin configurations and their energies, as well as prediction of energies.
Binary classification of states as ordered or disordered is used as a classification problem.

Consequently, the first part of this report derives the necessary mathematical tools for both regression and classification, including the back propagation algorithm for training neural networks. Different methods for classification and regression are then applied to the Ising data provided in\cite{mehta} and compared.

\section{Theory and methods}
See~\vref{sec:linreg} for a review of linear regression, including terminology and regression problem statement.
\subsection{The Ising model}
\begin{figure}[H]
    \centering
    \begin{tikzpicture}[thick, >=latex]
        \foreach \x in {1,2,...,14}{
            \pgfmathsetmacro{\s}{random(0,1)}
            \draw[->] (\x,\s) -- (\x,1-\s);
        }
    \end{tikzpicture}
    \caption{Example of a spin configuration in the one-dimensional Ising model. Spins pointing up and down represent \(s=+1\) and \(s=-1\).}
\end{figure}
The Ising model is a model for the energy of a system of binary spins, e.g.\ spins which only can take values \(\pm 1\).
Interactions are in this project assumed to be between nearest neighbours only, and the Hamiltonian is therefore given by
\begin{equation}
    H = -J\sum_{\ev{ij}} s_i s_j,
\end{equation}
where the sum runs over the nearest neighbours.
In one dimension, this reduces to
\begin{equation}
    H = - J\sum_{k=1}^{L} s_k s_{k+1},
\end{equation}
where \(s_{L+1}=s_1\) in order to enforce periodic boundary conditions.
According to the Boltzmann distribution, the probability of a state with energy \(H\) is proportional to \(\exp{-\beta H}\), where \(\beta=1/kT\) and \(T\) is the temperature.
At low temperatures, states with a low energy, i.e.\ ordered states, are preferred, while the probability distribution flattens for large temperature. Therefore, an unordered state is more likely at higher temperatures. The transition between preferring ordered and unordered states is called a \emph{phase transition}. While no phase transition exists for the one-dimensional Ising model, the two-dimensional Ising model has a phase transition at \(kT/J=\num{2.27}\).

\subsubsection{Regression problem}
The performance of the different regression models can be evaluated on data created from the Ising model by ``forgetting'' the true model with a constant coupling coefficient \(J\) and assuming that it is non-constant,
\begin{equation}
    H = -\sum_{i, j}J_{ij} s_i s_j.
\end{equation}
Given a system with \(L\) spins, this gives \(L^2\) basis functions on the form \(-s_i s_j\) and \(L^2\) coefficients \(J_{ij}\) to be determined by regression to find an estimator of the energy of a spin configuration. Both linear regression methods and neural networks will be applied.

\subsubsection{Classification problem}
A dataset of two-dimensional spin configurations has been made available in~\cite{mehta}. These configurations have been created at different temperatures and are therefore labelled as either ordered or unordered. Logistic regression and neural networks can be used for this task.

%  _             _     _   _
% | | ___   __ _(_)___| |_(_) ___
% | |/ _ \ / _` | / __| __| |/ __|
% | | (_) | (_| | \__ \ |_| | (__
% |_|\___/ \__, |_|___/\__|_|\___|
%          |___/
%  _ __ ___  __ _ _ __ ___  ___ ___(_) ___  _ __
% | '__/ _ \/ _` | '__/ _ \/ __/ __| |/ _ \| '_ \
% | | |  __/ (_| | | |  __/\__ \__ \ | (_) | | | |
% |_|  \___|\__, |_|  \___||___/___/_|\___/|_| |_|
%           |___/
\subsection{Binary logistic regression}\label{subsec:logreg}
Binary logistic regression is applied to problems where there are two possible outcomes, e.g. \(y_i=1\) or \(y_i=0\). Given an input \(\vec{x}_i\) (for example a spin configuration), a logistic regressor gives as output the probability \(p_i\) that \(y_i=1\).
The name logistic regression stems from the use of a logistic function as the value of \(p\), where the logistic function is applied to a linear combination of functions evaluated in the output, i.e.
\begin{equation}
    p_i = \frac{1}{1 + \exp{-\beta_j \phi_j(\vec{x}_i)}}.
\end{equation}
The matrix \(X\) consisting of \(X_{ij} = \phi_j(\vec{x}_i)\) can be created in the same manner as in linear regression, and the equation above is straightforwardly vectorised as
\begin{equation}
    \vec{p} = \frac{1}{1 + \exp{-X\vec{\beta}}}.
\end{equation}

\subsubsection{Cost function}
If a given output \(y_i\) is \(1\), \(p_i\) gives the likelihood of this result as predicted by the logistic regressor. Similarly, if \(y_i=0\), \(1-p_i\) gives the likelihood. This can be combined into \(l_i = p_i^{y_i} \qty(1-p_i)^{1-y_i}\). The product rule gives the total likelihood,
\begin{equation}
    L = \prod_{i} l_i = \prod_{i} p_i^{y_i} \qty(1-p_i)^{1-y_i},
\end{equation}
which should be maximised by the logistic regressor. Since the logarithm is a strictly monotonous function, one can instead choose to maximise
\begin{equation}
    \ln(L) = \sum_i y_i \ln(p_i) + \qty(1-y_i)\ln(1-p_i),
\end{equation}
a much nicer expression to differentiate. Finally, this can be turned into a minimisation problem with a minus sign, giving the final cost function
\begin{equation}
    Q(\vec{\beta}) = -\sum_i \qty(y_i\ln(p_i) + \qty(1-y_i)\ln(1-p_i)),
\end{equation}
called the cross-entropy.

\subsubsection{Minimisation}
Letting \(t_i=X_{ij}\beta_j = \phi_j(\vec{x}_i)\beta_j\) and using the result
\begin{equation}
    \dv{x}(\frac{1}{1+\exp{-x}}) = \frac{\exp{-x}}{\qty(1-\exp{-x})^2}
                                 = \frac{1 + \exp{-x} - 1}{\qty(1+\exp{-x})^2}
                                 = \frac{1}{1+\exp{-x}} - \frac{1}{\qty(1+\exp{-x})^2}
                                 = \frac{1}{1+\exp{-x}}\qty(1 - \frac{1}{1+\exp{-x}}),
\end{equation}
the derivative of the cost function is
\begin{equation}
    \pdv{Q}{\beta_k} = \pdv{Q}{p_j}\pdv{p_j}{\beta_k}
                     = \pdv{Q}{p_j}\pdv{p_j}{t_l}\pdv{t_l}{\beta_k}
                     = \sum_j \pdv{Q}{p_j}\pdv{p_j}{t_j}\pdv{t_j}{\beta_k}
                     = \sum_j \pdv{Q}{p_j}p_j\qty(1-p_j)X_{jk}.
\end{equation}
Further,
\begin{equation}
    \pdv{Q}{p_j} = - \frac{y_j}{p_j} + \frac{1-y_j}{1-p_j}
                 = \frac{-y_j\qty(1-p_j) + p_j\qty(1-y_j)}{p_j\qty(1-p_j)}
                 = \frac{p_j - y_j}{p_j\qty(1-p_j)},
\end{equation}
giving
\begin{equation}
    \pdv{Q}{\beta_k} = \qty(p_j - y_j) X_{jk},
\end{equation}
or, in matrix form,
\begin{equation}
    \nabla_{\vec{\beta}} Q = X^T \qty(\vec{p} - \vec{y}),
\end{equation}
with \(\vec{p} = 1/\qty(1+\exp{-X\vec{\beta}})\).
The equation \(\nabla Q = \vec{0}\) does not have an analytic solution, but it can be minimised with the standard minimisation methods.

\subsubsection{Regularisation}
As with linear regression, a logistic regressor can be regularised by adding a term to the cost function in the hope of reducing variance and overfitting.
One example is the same term as in Ridge regression,
\begin{equation}
    Q = - \sum_i \qty(y_i \ln(p_i) + (1-y_i)\ln(1-p_i)) + \tfrac{1}{2}\lambda \norm{\vec{\beta}}_2^2,
\end{equation}
which adds a simple term to the gradient of the cost function,
\begin{equation}
    \nabla_{\vec{\beta}} Q = X^T \qty(\vec{p} - \vec{y}) + \lambda \vec{\beta}.
\end{equation}


%                            _              _                      _
% _ __   ___ _   _ _ __ __ _| |  _ __   ___| |___      _____  _ __| | _____
%| '_ \ / _ \ | | | '__/ _` | | | '_ \ / _ \ __\ \ /\ / / _ \| '__| |/ / __|
%| | | |  __/ |_| | | | (_| | | | | | |  __/ |_ \ V  V / (_) | |  |   <\__ \
%|_| |_|\___|\__,_|_|  \__,_|_| |_| |_|\___|\__| \_/\_/ \___/|_|  |_|\_\___/
\subsection{Neural networks}
Neural networks can be used for both classification and regression.
They aim to be more accurate than linear and logistic regression by using a more complex set of functions for approximation.
A neural network is a series of affine transformations and activation functions, where the activation function may for example be a logistic function.

Each pair of one affine transformation and activation function is called a \emph{layer}. The number of outputs from a layer is called the number of \emph{neurons} in the layer. Mathematically, layer number \(l\) contains a weight-matrix \(W^l\), a bias vector \(\vec{b}^l\) and an activation function \(f^l\), takes input \(\vec{a}^{l-1}\), applies the affine transformation \(\vec{z}^l = W^l \vec{a}^{l-1} + \vec{b}^l\) and returns the output \(f^l(\vec{z}^l)\).

Prediction is done recursively. The first layer, called the input layer, applies its affine transformation to the input vector and then its activation function. The second layer receives the output of the first layer and repeats the process. This is repeated up to and including the final (output) layer, whose activation function is tailored to either classification or regression.

\subsubsection{Minimisation: Back-propagation}
Similarly to logistic regression, the cost function should be minimised using some sort of gradient method.
This requires taking the derivative of the cost function with respect to the fitting parameters, i.e.\ the weight matrices \(W^l\) and the bias vectors \(\vec{b}^l\) for neural networks.
Since the prediction is calculated as a series of compositions of complicated functions, it is not possible to derive closed-form expressions of the derivatives of the cost function with respect to weights and biases.
Instead, one can find the derivative with respect to the weights and biases of the \emph{last} layer, and then derive a recursion relation which gives the derivatives at earlier layers as well. This is the famous \emph{back-propagation} algorithm.

First, the derivatives of the cost function with respect to the biases can be calculated via the chain rule,
\begin{equation}
    \pdv{Q}{b_j^l} = \pdv{Q}{z_k^l}\pdv{z_k^l}{b_j^l} = \pdv{Q}{z_k^l}\pdv{b_j^l}(W_{ki}^l a_i^{l-1} + b_k^l) = \pdv{Q}{z_k^l}\delta_{jk}
    = \pdv{Q}{z_j^l} \eqqcolon \delta_j^l.
\end{equation}
Applying the chain rule again, \(\delta_j^l\) is rewritten as
\begin{equation}
    \delta_j^l = \pdv{Q}{z_j^l} = \pdv{Q}{a_k^l}\pdv{a_k^l}{z_j^l} = \pdv{Q}{a_k^l}\pdv{f^l\qty(z_k^l)}{z_j^l}
               = \pdv{Q}{a_j^l}\pdv{f^l}{z_j^l}.
\end{equation}
When doing regression, \(f^L\) will usually be the identity, giving \(1\) as the last partial derivative. This is not the case in classification, nor in the earlier layers.

Additionally, the derivatives with respect to the weight matrix elements are
\begin{equation}
    \pdv{Q}{W_{jk}^l} = \pdv{Q}{z_i^l}\pdv{z_i^l}{W_{jk}^l} = \delta_i^l \pdv{W_{jk}^l}\qty(W_{im}^l a_m^{l-1} + b_i^l)
                      = \delta_j^l a_k^{l-1}.
\end{equation}
These equations can be combined to give the derivatives of the cost function with respect to the weights and biases at the last layer, but does not say anything about the gradients at earlier layers, which would require knowledge of \(\delta_j^l\) for \(l<L\).
This relation can be derived through one final application of the chain rule,
\begin{equation}
    \delta_j^l = \pdv{Q}{z_j^l} = \pdv{Q}{z_k^{l+1}}\pdv{z_k^{l+1}}{z_j^l}
               = \delta_k^{l+1} \pdv{z_k^{l+1}}{a_i^l}\pdv{a_i^l}{z_j^{l}}
               = \delta_k^{l+1} \pdv{a_i^l}\qty(W_{km}^{l+1}a_m^l + b_k^l) \pdv{f^l(z_i^l)}{z_j^{l}}
               = \delta_k^{l+1} W_{kj}^{l+1} {f^l} '(z_j^l).
\end{equation}

The above results can be summarised on a matrix form as
\begin{equation}
    \vec{\delta}^{\,L} = \nabla_{\vec{a}^L}Q \circ {f^L}'(\vec{z}^L),
\end{equation}
which is easily calculated given a cost function \(Q\) and final activation function \(f^L\) (\(\circ\) denotes the element-wise product),
\begin{equation}
    \vec{\delta}^{\,l} = \qty(W^{l+1, T}\vec{\delta}^{\,l+1})\circ {f^l}'(\vec{z}^{\,l}),
\end{equation}
which can be calculated recursively once \(\vec{\delta}^L\) is known from the previous equation, and finally the gradients
\begin{equation}
    \nabla_{\vec{b}^l}Q = \vec{\delta}^{\,l} \qquad \text{and}\qquad \nabla_{W^l}Q = \vec{\delta}^{\,l} \otimes \vec{a}^{l-1}.
\end{equation}
Together, these four equations give a complete recipe of how to calculate the gradients of the cost function in a neural network.

\subsubsection{Activation functions}
The simplest activation function is the identity, whose derivative is \(1\). This is often used in the output layer for regression problems.
Another activation function, which historically has been widely used in the intermediate (hidden) layers of the neural network, is the sigmoid or logistic function, whose derivative was shown earlier to be
\begin{equation}
    f'(z) = f(z)(1-f(z)).
\end{equation}
Other activation functions include the hyperbolic tangent, with derivative
\begin{equation}
    f'(z) = 1 - f(z)^2,
\end{equation}
and the rectified linear unit (RELU),
\begin{equation}
    f(z) = \begin{cases}z, & z \geq 0 \\ 0, & z < 0 \end{cases}
    \implies
    f'(z) = \begin{cases}1, & z \geq 0 \\ 0, & z < 0 \end{cases}.
\end{equation}
The rectified linear unit usually performs the best\cite{mehta} and is used throughout this project (except in the output layer for classification).

\subsubsection{Cost functions and output layers}
When applying a neural network to regression problems, the natural choice of cost function is simply the squared deviation from the known outputs,
\begin{equation}
    Q = \tfrac{1}{2}\norm{\vec{a}^L - \vec{y}}_2^2,
\end{equation}
with the gradient
\begin{equation}
    \nabla_{\vec{a}^L} Q = \vec{a}^L - \vec{y}.
\end{equation}
Regressing neural networks usually use the identity as the activation function for the output layer, giving
\begin{equation}
    {f^L}'(\vec{z}^L) = \vec{1}
\end{equation}
and
\begin{equation}
    \vec{\delta}^{\,L} = \vec{a}^L - \vec{y}.
\end{equation}

Binary classification problems such as the one of the two-dimensional Ising model may use the same cost function as that of logistic regression,with the logistic function used as the activation function of the output layer in order to obtain a probability \(a^L\) between zero and one.
The cost function is then
\begin{equation}
    Q = -\qty(y\ln(a^L) + (1 - y)\ln(1-a^L))
\end{equation}
with
\begin{equation}
    \nabla_{\vec{a}^L} Q = \pdv{Q}{a^L} = -\frac{y}{a^L} - \frac{1-y}{1-a^L}
                         = \frac{a^L - y}{a^L\qty(1-a^L)}
\end{equation}
giving
\begin{equation}
    \delta^L = \pdv{Q}{a^L} \pdv{f^L}{z^L}
             = \frac{a^L - y}{a^L\qty(1-a^L)} \cdot f^L(z^L)\qty(1-f^L(z^L))
             = a^L - y.
\end{equation}

\subsubsection{Implementation}
As in the previous project, I have chosen to use an object-oriented approach and Fortran. This combination resulted in general, easy-to-use and efficient code for regression methods. The neural network is therefore similarly organised.

The most important class is the \lstinline{neural_network} class. This contains methods for training, prediction and back propagation, which call corresponding methods on each element in the network's array of \lstinline{layer} objects. Each \lstinline{layer} has a weight matrix, a bias vector, some temporary output and input vectors, and an \lstinline{activation_function} object. Sublasses of \lstinline{activation_function} have been implemented for both the sigmoid function and the rectifying linear unit. These implement a method for evaluation and differentiation. Giving the last \lstinline{layer} instances of different \lstinline{activation_function} subclasses tailors the neural network to either classification or regression.

Coarrays, introduced in Fortran2008, were used to parallelise the neural network. Each batch in the stochastic batch gradient descent is divided among the available processes. When all processes have finished with their given amount of work, the resulting gradients are summed and used to update the weights. Synchronisation therefore only takes place between each batch. For large batch sizes, the speedup of parallelisation should be almost perfect.


%      _             _               _   _
%  ___| |_ ___   ___| |__   __ _ ___| |_(_) ___
% / __| __/ _ \ / __| '_ \ / _` / __| __| |/ __|
% \__ \ || (_) | (__| | | | (_| \__ \ |_| | (__
% |___/\__\___/ \___|_| |_|\__,_|___/\__|_|\___| _
%  _ __ ___ (_)_ __ (_)_ __ ___ (_)___  __ _| |_(_) ___  _ __
% | '_ ` _ \| | '_ \| | '_ ` _ \| / __|/ _` | __| |/ _ \| '_ \
% | | | | | | | | | | | | | | | | \__ \ (_| | |_| | (_) | | | |
% |_| |_| |_|_|_| |_|_|_| |_| |_|_|___/\__,_|\__|_|\___/|_| |_|
\subsection{Stochastic minimisation algorithms}
See~\vref{subsec:min} for a review of gradient descent and Newton's method.

Gradient descent and Newton's method are often suboptimal because of their tendency to get stuck in local minima.
A simple extension is the stochastic batch-gradient descent, which selects batches of inputs from the training data with replacement and trains either a neural network or a logistic regressor on each batch. If \(b\) batches are chosen out of the \(N\) training data sets, there are \(N/b\) input items to be chosen with replacement for each batch, and the gradient descent update step is
\begin{equation}
    \vec{\beta}_{k+1} = \vec{\beta}_k - \alpha \nabla_{\vec{\beta}} Q\qty(X_{\qty[\vec{\imath},:]},\vec{y}_{\vec{\imath}}),
\end{equation}
where \(Q\qty(X_{\qty[\vec{\imath},:]},\vec{y}_{\vec{\imath}})\) denotes the gradient applied to \(\vec{x}_j\) for all \(j\in\vec{\imath}\), \(\vec{\imath}\) being a set of \(N/b\) indices.

A further improvement is to add a so-called momentum, i.e.\ use a linear combination of the current and previous gradient rather than just using the current gradient,
\begin{equation}
    \vec{v}_{k} = \gamma \vec{v}_{k-1} + \alpha Q\qty(X_{\qty[\vec{\imath},:]},\vec{y}_{\vec{\imath}}), \qquad
    \vec{\beta}_{k+1} = \vec{\beta}_k - \vec{v}_k.
\end{equation}

%                      _ _
%  _ __ ___  ___ _   _| | |_ ___
% | '__/ _ \/ __| | | | | __/ __|
% | | |  __/\__ \ |_| | | |_\__ \
% |_|  \___||___/\__,_|_|\__|___/
\section{Results and discussion}
\subsection{Linear regression}\label{subsec:linregresults}
One-dimensional binary lattices were generated randomly, and their energies were calculated.
The regression methods were told to approximate the energies as linear combinations of spin-spin products, \(E=-J_{ij}s_i s_j\).
An interesting feature of this regression problem is the non-uniqueness of the coupling matrix.
While the energy is generated from \(E=-\sum_j J_{j,j+1}s_j s_{j+1}\), any matrix \(J\) which fulfils \(J_{j,j+1} + J_{j,j-1}=1\) and \(J_{ij} = -J_{ji}\) for \(i\neq j\pm1\) (requiring \(J_{ii}=0\)) will give the same energy.

Widely different behaviours are seen for the three regression algorithms.
Ordinary least squares gives strange results, while Ridge and LASSO regression give good, but different, results.
Ridge regression, whose cost function includes the sum of the squares of the coefficients \(\beta_j\), gives a symmetric coupling matrix because \(\num{0.5}^2+\num{0.5}^2\) is the smallest squared sum of two numbers which add up to one.
The resulting performance is an \(R^2\) score around \(\num{0.8}\) when \(\lambda\) is not chosen too large.

On the other hand, LASSO regression is able to perfectly reproduce the asymmetric coupling matrix used to generate the data with the correct choice of regularisation parameter, resulting in an \(R^2\) score of \(1\). Using more training samples reduces the sensitivity to \(\lambda\), while the peak performance is the same for \(\num{400}\) and \(\num{1000}\) samples.

When the number of samples reaches the number of parameters, \(L^2=\num{1600}\), Ridge regression gives perfect energies because the underlying model of the energy is the same as the model used for fitting. Reducing the number of samples reduces the accuracy. LASSO regression, on the other hand, manages to find a perfect fit for certain values of \(\lambda\) even with \(\num{400}\) samples.

Coupling matrices showing transitions and other interesting features are shown below. A complete set is given in~\vref{fig:morecouplings} and beyond.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            thick,
            width=6in, height=3in,
            legend style={draw=none, fill=none, at={(0.15,0.1)}, anchor=south west},
            legend cell align=left,
            xmode=log,
            xlabel={\(\lambda\)},
            ylabel={\(R^2\) score},
        ]
            \addplot+ table[y index=1] {data/mse_Ridge_1000.dat};
            \addlegendentry{Ridge, \(\num{1000}\) states};
            \addplot+ table[y index=1] {data/mse_Ridge_1600.dat};
            \addlegendentry{Ridge, \(\num{1600}\) states};
            \addplot+ table[y index=1] {data/mse_LASSO_400.dat};
            \addlegendentry{LASSO, \(\num{400}\) states};
            \addplot+ table[y index=1] {data/mse_LASSO_1000.dat};
            \addlegendentry{LASSO, \(\num{1000}\) states};
        \end{axis}
    \end{tikzpicture}
    \caption{\(R^2\) score on test data. While the regularisation of Ridge regression does not significantly improve the performance, as illustrated by the constant coupling matrices in~\vref{fig:coupling}, the correct choice of regularisation parameter enables LASSO to give perfect predictive ability. This corresponds well with the coupling matrices shown in~\ref{fig:coupling} and onwards, which show that LASSO regularisation gives the same, asymmetric coupling matrices as those of the underlying model.}\label{fig:r2}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            thick,
            width=4in, height=3in,
            xmode=log, ymode=log,
            legend style={draw=none, fill=none, at={(1.0,0.02)}, anchor=south west},
            legend cell align=left,
            xlabel={\(\lambda\)},
            ylabel={Error},
        ]
            \addplot+ table[y index=2] {data/mse_Ridge_1000.dat};
            \addlegendentry{MSE (Ridge, \(\num{1000}\) states)};
            \addplot+ table[y index=3] {data/mse_Ridge_1000.dat};
            \addlegendentry{Bias (Ridge, \(\num{1000}\) states)};
            \addplot+ table[y index=4] {data/mse_Ridge_1000.dat};
            \addlegendentry{Variance (Ridge, \(\num{1000}\) states)};
            \addplot+ table[y index=2] {data/mse_Ridge_1600.dat};
            \addlegendentry{MSE (Ridge, \(\num{1600}\) states)};
            \addplot+ table[y index=3] {data/mse_Ridge_1600.dat};
            \addlegendentry{Bias (Ridge, \(\num{1600}\) states)};
            \addplot+ table[y index=4] {data/mse_Ridge_1600.dat};
            \addlegendentry{Variance (Ridge, \(\num{1600}\) states)};
            \addplot+ table[y index=2] {data/mse_LASSO_400.dat};
            \addlegendentry{MSE (LASSO, \(\num{400}\) states)};
            \addplot+ table[y index=3] {data/mse_LASSO_400.dat};
            \addlegendentry{Bias (LASSO, \(\num{400}\) states)};
            \addplot+ table[y index=4] {data/mse_LASSO_400.dat};
            \addlegendentry{Variance (LASSO, \(\num{400}\) states)};
            \addplot+ table[y index=2] {data/mse_LASSO_1000.dat};
            \addlegendentry{MSE (LASSO, \(\num{1000}\) states)};
            \addplot+ table[y index=3] {data/mse_LASSO_1000.dat};
            \addlegendentry{Bias (LASSO, \(\num{1000}\) states)};
            \addplot+ table[y index=4] {data/mse_LASSO_1000.dat};
            \addlegendentry{Variance (LASSO, \(\num{1000}\) states)};
        \end{axis}
    \end{tikzpicture}
    \caption{Bias-variance decomposition of the mean squared error for Ridge and LASSO regression. The bias and variance are expected to add up to the mean squared error, which is confirmed by the plot. Further, using a regularisation parameter \(\lambda\) which is too large causes the mean squared error to increase because the approximating model becomes unable to fit the data, i.e.\ the bias increases, and the reduction of variance can not compensate sufficiently. As seen also in~\vref{fig:r2}, LASSO outperforms Ridge regression for certain values of \(\lambda\) when fewer samples than \(L^2\) is used.}\label{fig:mse}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}\centering
        \includegraphics[width=0.8\textwidth]{data/J_ols_1000_1.png}
        \caption{\(\num{1000}\) states.}
    \end{subfigure}
    \begin{subfigure}[t]{0.45\textwidth}\centering
        \includegraphics[width=0.8\textwidth]{data/J_ols_1600_1.png}
        \caption{\(\num{1600}\) states.}
    \end{subfigure}
    \caption{Coupling matrix for ordinary least squares regression. White is negative one, black is one. It is clear that the lack of regularisation causes overfitting when few states are used as input. Additionally, ordinary least squares and Ridge regression find a coupling matrix whose super and sub diagonals are equal. The strange lower right quarter of the coupling matrix can be caused by either something wrong which I have done, or the use of \lstinline{dgelsd}, which finds a singular value decomposition via the divide-and-conquer algorithm. Notice that most of the noise (that is, non-zero elements other than on the sub and super diagonals) is antisymmetric, so that these elements cancel and do not affect the energy.}
\end{figure}

\begin{luacode*}
i = 1
for line in io.lines("data/Ridge_lambdas.dat") do
    if i<3 or i==5 or i>7 then
        s = ""
            .. [[\begin{figure}[H] ]]
            .. [[\centering]]
            .. [[\begin{subfigure}[t]{0.2\textwidth} \centering]]
                .. [[\includegraphics[height=0.8\textwidth]{data/J_Ridge_1000_]] .. tostring(i) .. [[.png}]]
                .. [[\caption{Ridge regression, \(\num{1000}\) training states.}]]
            .. [[\end{subfigure}\hspace{0.04\textwidth}]]
            .. [[\begin{subfigure}[t]{0.2\textwidth} \centering]]
                .. [[\includegraphics[height=0.8\textwidth]{data/J_Ridge_1600_]] .. tostring(i) .. [[.png}]]
                .. [[\caption{Ridge regression, \(\num{1600}\) training states.}]]
            .. [[\end{subfigure}\hspace{0.04\textwidth}]]
            .. [[\begin{subfigure}[t]{0.2\textwidth} \centering]]
                .. [[\includegraphics[height=0.8\textwidth]{data/J_LASSO_400_]] .. tostring(i) .. [[.png}]]
                .. [[\caption{LASSO regression, \(\num{400}\) training states.}]]
            .. [[\end{subfigure}\hspace{0.04\textwidth}]]
            .. [[\begin{subfigure}[t]{0.2\textwidth} \centering]]
                .. [[\includegraphics[height=0.8\textwidth]{data/J_LASSO_1000_]] .. tostring(i) .. [[.png}]]
                .. [[\caption{LASSO regression, \(\num{1000}\) training states.}]]
            .. [[\end{subfigure}]]
            .. [[\caption{Coupling matrices for \(\lambda = \num{]] .. line ..[[}\).}]]
        if i == 1 then s = s .. [[ \label{fig:coupling} ]] end
        s = s .. [[\end{figure}]]
        tex.sprint(s)
    end
    ::next::
    i = i + 1
end
\end{luacode*}

\subsection{Logistic regression}
Mehta et.\ al.\cite{mehta} have provided a data set with \(\num{160000}\) spin configurations on a \(40\times40\) lattice. The states are labelled as ordered or disordered. Of these, \(\num{30000}\) are considered \emph{critical}, i.e.\ in the range between ordered and disordered (this phase transition is sharp only in the limit of infinite lattice size). The non-critical states were divided into training and test data, while the critical states were kept as an extra test set.

There are many parameters to be chosen in logistic regression. I chose to vary regularisation, learning rate and momentum to determine the optimal combination of parameters. The batch size could also have been varied, but this was kept fixed for simplicity and reduction of runtime. Additionally, the batch size only determines the rate of convergence, while wrong choices for the other parameters often lead to no convergence at all. For each \(\lambda\), an optimal learning rate and momentum was determined through a (parallel) parameter sweep.

\begin{table}[H]
    \centering
    \caption{Accuracy on training and test data, as well as the critical states, for a variety of regularisations \(\lambda\). Optimal momentums and learning rates are estimated through a simple parameter sweep. The number of stochastic gradient descent iterations was limited to \(100\), while the batch size was \(32\). Other prints from the program show that the choice of hyperparameters such as learning rate is very important for logistic regression to be better than guessing.}
    \pgfplotstabletypeset[
        columns/lambda/.style={column name={$\lambda$},
            dec sep align,
            sci, sci zerofill,
            precision=1,
            string replace={0.E+00}{}
        },
        columns/{Learning rate}/.style={sci, sci zerofill, precision=1},
        fixed, precision=2, fixed zerofill
    ]{data/logreg_table.dat}
\end{table}

\subsection{Neural network regression}
\subsubsection{Rediscovering the Ising model}
A neural network with only an output layer and no activation function is identical to a linear regressor, except for the use of a minimisation algorithm instead of closed form solutions.
If the same input is used as in~\vref{subsec:linregresults}, the weight matrix of such a neural network should be equal to the coupling matrix of the Ising model.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            thick,
            width=6in, height=3in,
            legend style={draw=none, fill=none, at={(0.98,0.02)}, anchor=south east},
            legend cell align=left,
            xmode=log,
            xlabel={epochs},
            ylabel={\(R^2\) score},
        ]
            \addplot+ table[y index=1] {data/reg_nn_test_couplings.dat};
            \addlegendentry{\(\alpha=\num{0.001}\)};
            \addplot+ table[y index=2] {data/reg_nn_test_couplings.dat};
            \addlegendentry{\(\alpha=\num{0.01}\)};
            \addplot+ table[y index=3] {data/reg_nn_test_couplings.dat};
            \addlegendentry{\(\alpha=\num{0.04}\)};
        \end{axis}
    \end{tikzpicture}
    \caption{\(R^2\) score on test data as a function of the number of epochs, i.e.\ the number of stochastic gradient descent iterations. \(\num{3000}\) states were used for training with a batch size of \(300\). With the right learning rate, the neural network converges quickly to the correct solution, while a badly chosen learning rate leads to slower convergence or even divergence. A learning rate of \(\num{0.05}\) ruins everything (not shown).}\label{fig:r2nn}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\textwidth]{data/J_nn_1.png}
    \caption{Weight matrix of the neural network's only layer, which should be equal to the coupling matrix \(J\). The neural network gives a symmetric matrix with weights \(\num{0.5}\) on the super and sub diagonals.}
\end{figure}

\begin{table}[H]
    \centering
    \caption{Optimal combination of learning rate and batch size for a few different values of \(\lambda\). The given numbers of epochs and training time are determined by registering when the neural network gives \(R^2\geq \num{0.98}\) on test data. Optimal parameters are defined as the parameters which give the shortest \emph{training time}. A batch size of \(1\) always gives the smallest number of required epochs, but does not utilise the parallelisation of the neural network. These results are generated on a \(4\)-core processor. Using more cores would lead to a larger optimal batch size. A suitable choice of regularisation parameter is occasionally seen to reduce training time and the required number of epochs, while too strong regularisation keeps the neural network from converging sufficiently.}
    \pgfplotstabletypeset[
        columns/lambda/.style={column name={$\lambda$},
            dec sep align,
            sci, sci zerofill,
            precision=1,
            string replace={0.0E+00}{}
        },
        columns/{Learning rate}/.style={sci, sci zerofill, precision=1,string replace={0.0E+00}{}},
        columns/{Training time [s]}/.style={sci, sci zerofill, precision=1,string replace={0.0E+00}{}},
        columns/{Epochs for convergence}/.style={string replace={0}{}},
        columns/{Batch size}/.style={string replace={0}{}},
        fixed, precision=0, fixed zerofill
    ]{data/reg_nn_convergence.dat}
\end{table}

\subsubsection{Energy from a black body}
The previous exercise did not use the full power of neural networks, i.e.\ their ability to take in some input, e.g.\ a spin system, and spit out some output, e.g.\ energy, without any knowledge of how the output should depend on the input --- a ``black box''-behaviour. Instead, the neural network was turned into a linear regression machine because it was told to apply an affine transformation to fit data which was known to be linear.

It is therefore interesting to see whether the neural network can predict energies when it is just given the spin configuration and no other knowledge. Since the energy has a non-linear dependence on the spins, this requires more than one layer. As seen in~\vref{fig:r2nnspin}, a modest neural network with one hidden layer with \(\num{400}\) neurons achieves an \(R^2\) score above \(\num{0.98}\). Increasing the number of layers does not contribute significantly, since the energy is a linear combination of products of spins, requiring a total of two layers.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            thick,
            width=6in, height=3in,
            legend style={draw=none, fill=none, at={(0.98,0.02)}, anchor=south east},
            legend cell align=left,
            xmode=log,
            xlabel={epochs},
            ylabel={\(R^2\) score},
        ]
            \addplot+ table[y index=1] {data/reg_nn_test_spins.dat};
            \addlegendentry{\(\alpha=\num{0.001}\)};
            \addplot+ table[y index=2] {data/reg_nn_test_spins.dat};
            \addlegendentry{\(\alpha=\num{0.01}\)};
            \addplot+ table[y index=3] {data/reg_nn_test_spins.dat};
            \addlegendentry{\(\alpha=\num{0.04}\)};
        \end{axis}
    \end{tikzpicture}
    \caption{\(R^2\) score on training data as a function of the number of epochs, i.e.\ the number of stochastic gradient descent iterations. \(\num{10000}\) states were used for training with a batch size of \(20\), \(\lambda=\num{0.001}\) and \(L=40\). The neural network consists of one hidden layer with \(400\) neurons. While a learning rate of \(\num{0.04}\) initially converges the fastest, a smaller learning rate results in a better final \(R^2\) score. An adaptive scheme may therefore perform better.}\label{fig:r2nnspin}
\end{figure}

\subsection{Neural network classification}
The logistic regression in~\vref{subsec:logreg} showed mediocre performance, never achieving an accuracy above approximately \(\SI{70}{\percent}\). A neural network, with its series of affine transformations and activation functions, has a much more complex approximating function, more degrees of freedom and therefore a better chance of reaching a near-perfect predicting performance.

\Vref*{fig:nnclass} shows that this is indeed the case. Whereas logistic regression required all \(\num{130000}\) spin configurations to reach an accuracy of \(\SI{70}{\percent}\), simple neural networks converge to perfect accuracy on test data within few epochs when using one tenth of the spin configurations. In addition to the improved accuracy, this means that the neural networks are more efficient despite their higher complexity, because they need less data and fewer epochs.

Interestingly, a deeper neural network does not seem to converge in fewer epochs. This may be due to the fact that the order/disorder in a lattice configuration only depends on quantities like the magnetisation, which is a first order polynomials of the spins (the sum), and the energy, which is a second order polynomial. A second order approximation only requires one hidden layer, since both the hidden and the output layer apply an affine transformation.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            thick,
            width=6in, height=3in,
            legend style={draw=none, fill=none, at={(0.98,0.02)}, anchor=south east},
            legend cell align=left,
            xmode=log,
            xlabel={epochs},
            ylabel={\(R^2\) score},
        ]
            \addplot+ table[y index=1] {data/class_test.dat};
            \addlegendentry{\(\alpha=\num{0.001}\), \(10\) neurons};
            \addplot+ table[y index=2] {data/class_test.dat};
            \addlegendentry{\(\alpha=\num{0.001}\),  \(10\times10\) neurons};
            \addplot+ table[y index=3] {data/class_test.dat};
            \addlegendentry{\(\alpha=\num{0.001}\), \(100\) neurons};
            \addplot+ table[y index=4] {data/class_test.dat};
            \addlegendentry{\(\alpha=\num{0.01}\), \(10\) neurons};
            \addplot+ table[y index=5] {data/class_test.dat};
            \addlegendentry{\(\alpha=\num{0.01}\),  \(10\times10\) neurons};
            \addplot+ table[y index=6] {data/class_test.dat};
            \addlegendentry{\(\alpha=\num{0.01}\), \(100\) neurons};
        \end{axis}
    \end{tikzpicture}
    \caption{Accuracy on test data as a function of the number of epochs for a few neural network architectures and two different learning rates with batch size \(20\) and \(\lambda=\num{0.001}\). Only one tenth of the data provided by Mehta et.\ al.\cite{mehta} was used. This amount of data rendered logistic regression useless, while the neural networks quickly converge to perfect accuracy with as little as one hidden layer with \(10\) neurons.}\label{fig:nnclass}
\end{figure}


\clearpage
\nocite{*}
\printbibliography{}
\addcontentsline{toc}{section}{\bibname}
%                                   _ _
%   __ _ _ __  _ __   ___ _ __   __| (_)_  __
%  / _` | '_ \| '_ \ / _ \ '_ \ / _` | \ \/ /
% | (_| | |_) | |_) |  __/ | | | (_| | |>  <
%  \__,_| .__/| .__/ \___|_| |_|\__,_|_/_/\_\
%       |_|   |_|
\clearpage
\appendix
\part*{Appendix}
\addcontentsline{toc}{part}{Appendix}
\renewcommand{\thesection}{\Alph{section}}
\labelformat{section}{appendix~#1}
\labelformat{subsection}{appendix~#1}

\section{More coupling matrices}
\begin{luacode*}
i = 1
for line in io.lines("data/Ridge_lambdas.dat") do
    s = ""
        .. [[\begin{figure}[H] ]]
        .. [[\centering]]
        .. [[\begin{subfigure}[t]{0.2\textwidth} \centering]]
            .. [[\includegraphics[height=0.8\textwidth]{data/J_Ridge_1000_]] .. tostring(i) .. [[.png}]]
            .. [[\caption{Ridge regression, \(\num{1000}\) training states.}]]
        .. [[\end{subfigure}\hspace{0.04\textwidth}]]
        .. [[\begin{subfigure}[t]{0.2\textwidth} \centering]]
            .. [[\includegraphics[height=0.8\textwidth]{data/J_Ridge_1600_]] .. tostring(i) .. [[.png}]]
            .. [[\caption{Ridge regression, \(\num{1600}\) training states.}]]
        .. [[\end{subfigure}\hspace{0.04\textwidth}]]
        .. [[\begin{subfigure}[t]{0.2\textwidth} \centering]]
            .. [[\includegraphics[height=0.8\textwidth]{data/J_LASSO_400_]] .. tostring(i) .. [[.png}]]
            .. [[\caption{LASSO regression, \(\num{400}\) training states.}]]
        .. [[\end{subfigure}\hspace{0.04\textwidth}]]
        .. [[\begin{subfigure}[t]{0.2\textwidth} \centering]]
            .. [[\includegraphics[height=0.8\textwidth]{data/J_LASSO_1000_]] .. tostring(i) .. [[.png}]]
            .. [[\caption{LASSO regression, \(\num{1000}\) training states.}]]
        .. [[\end{subfigure}]]
        .. [[\caption{Coupling matrices for \(\lambda = \num{]] .. line ..[[}\).}]]
    if i == 1 then s = s .. [[ \label{fig:morecouplings} ]] end
    s = s .. [[\end{figure}]]
    tex.sprint(s)
    ::next::
    i = i + 1
end
\end{luacode*}

\section{Review of linear regression}\label{sec:linreg}
This section is mainly a copy-paste from project 1 and is included for reference.
\subsection{Fitting problem statement}
The general problem is to fit a given set of data \(D=\qty{(\vec{x}_i,y_i)}_{i=1}^N\), where \(\vec{x}_i\) are inputs of some dimensionality (\(2\) in this project) and \(y_i\) are scalar outputs.
A set of basis functions \(\qty{\phi_j}_{j=1}^p\) is chosen, and the goal is to approximate the input data with the linear combination \(\beta_j \phi_j(\vec{x})\).
If the data set were generated by such a linear combination, there would exist parameters \(\qty{\beta_j}\) such that \(\beta_j\phi_j(\vec{x}_i)=y_i\), which can be rewritten as the matrix equation
\begin{equation}
    X\vec{\beta} = \vec{y},
\end{equation}
where \(X\in\mathbb{R}^{N\times p}\) is the matrix with elements \(x_{ij}=\phi_j(\vec{x}_i)\), and \(\vec{\beta}\) and \(\vec{y}\) are the vectors with elements \(\beta_j\) and \(y_i\).

In general, it will not be possible to find parameters \(\beta_j\) which fulfill this, since the data set will not be generated from the simple basis functions.
Consequently, one must find the \(\beta_j\)s which in some sense make \(X\vec{\beta}\) as close to \(\vec{y}\) as possible.
The way in which to measure deviation from the perfect solution is called the \emph{cost function}, frequently written \(Q(\vec{\beta};D)\).
Regression methods differ in the choice of cost function, while their aim is always to find the parameters \(\beta_j\) which minimise the cost function.

When a regression method has been used to find \(\vec{\beta}\), predicted values are denoted \(\tilde{y}_i = X_{ij}\beta_j\), while the original, exact values are denoted \(y_i\).

%                _ _                          _                _
%   ___  _ __ __| (_)_ __   __ _ _ __ _   _  | | ___  __ _ ___| |_
%  / _ \| '__/ _` | | '_ \ / _` | '__| | | | | |/ _ \/ _` / __| __|
% | (_) | | | (_| | | | | | (_| | |  | |_| | | |  __/ (_| \__ \ |_
%  \___/|_|  \__,_|_|_| |_|\__,_|_|   \__, | |_|\___|\__,_|___/\__|
%  ___  __ _ _   _  __ _ _ __ ___  ___|___/
% / __|/ _` | | | |/ _` | '__/ _ \/ __|
% \__ \ (_| | |_| | (_| | | |  __/\__ \
% |___/\__, |\__,_|\__,_|_|  \___||___/
%         |_|
\subsection{Ordinary Least Squares}
The ordinary least squares method is the simplest and most intuitive, since its cost function is simply the square of the error made by the fit,
\begin{equation}
    Q(\beta;D) = \norm{\vec{y}-X\vec{\beta}}_2^2,
\end{equation}
where \(\norm{\cdot}_p\) denotes the usual \(p\)-norm.

\subsubsection{Geometric view}
The geometric view of the equation \(X\vec{\beta}=\vec{y}\) is to find the linear combination of the columns of \(X\) equal to \(\vec{y}\).
This is not necessarily possible if the columns of \(X\) do not span \(\mathbb{R}^N\), which is not possible if \(p<N\).
Ordinary least squares seeks to find the linear combination of the columns of \(X\) as close to \(\vec{y}\) as possible.
This is achieved when \(X\vec{\beta}\) is equal to the projection of \(\vec{y}\) onto the column space of \(X\), i.e.
\begin{equation}
    X\vec{\beta} = \Proj_{\Col{X}}{\vec{y}}.
\end{equation}
By construction, this equation will always have a solution, although it may not be unique if the columns of \(X\) are not linearly independent.
Since \(X\vec{\beta}\) is the projection of \(\vec{y}\) onto the column space of \(X\), the error, \(\vec{y}-X\vec{\beta}\), is orthogonal to the columns of \(X\), i.e.\ the rows of \(X^T\).
Consequently,
\begin{equation}
    X^T\qty(\vec{y}-X\vec{\beta}) = \vec{0}
    \implies
    X^T X \vec{\beta} = X^T \vec{y},\label{eq:olsnormal}
\end{equation}
which is a simple linear set of equations which can be solved for \(\vec{\beta}\).
This set of equations is called the \emph{normal equations}.
If \(X\) is non-singular, the minimisation problem in ordinary least squares has the closed form solution
\begin{equation}
    \vec{\beta}_{\text{OLS}} = \qty(X^T X)^{-1} X^T \vec{y}.
\end{equation}

\subsubsection{Minimisation view}
The cost function for ordinary least squares can be written as
\begin{equation}
    Q(\beta;D) = \norm{\vec{y}-X\vec{\beta}}_2^2
               = \sum_{i=1}^N \qty(y_i - X_{ij}\beta_j)^2.
\end{equation}
When this is minimised, \(\nabla Q = \vec{0}\), where the gradient denotes differentiation with respect to the parameters \(\beta_j\).
The components of the gradient can straightforwardly be calculated from the cost function,
\begin{equation}
    \nabla_k Q = \pdv{\beta_k}(\qty(y_i - X_{ij}\beta_j)\qty(y_i - X_{ij}\beta_j))
               = 2\qty(y_i-X_{ij}\beta_j) \pdv{\beta_k}(y_i-X_{ij}\beta_j)
               = -2X_{ik}\qty(y_i-X_{ij}\beta_j),\label{eq:olsgradcomp}
\end{equation}
which can be rewritten on vector form as
\begin{equation}
    \nabla Q = -2X^T\qty(\vec{y}-X\vec{\beta}).\label{eq:olsgrad}
\end{equation}
Setting \(\nabla Q = \vec{0}\) gives~\vref{eq:olsnormal}.

Ordinary least squares has been implemented by calling \lstinline{dgelss}, which uses a singular value decomposition to prevent numerical instabilities.

%      _     _
% _ __(_) __| | __ _  ___
%| '__| |/ _` |/ _` |/ _ \
%| |  | | (_| | (_| |  __/
%|_|  |_|\__,_|\__, |\___|
%              |___/
\subsection{Ridge regression}
The cost function used in Ridge regression is
\begin{equation}
    Q_\lambda(\vec{\beta};D) = \norm{\vec{y}-X\vec{\beta}}_2^2 + \lambda \norm{\vec{\beta}}_2^2
                             = \sum_{i=1}^N \qty(y_i - X_{ij}\beta_j)^2 + \lambda\sum_{i=1}^p \beta_i^2,
\end{equation}
which penalises solution vectors \(\vec{\beta}\) where some coefficients are large.
\(\lambda\) is a parameter which should be chosen with great care.
The error is as small as possible for \(\lambda=0\), as this will reproduce the solution found by ordinary least squares, but a non-zero value of \(\lambda\) will give \(\beta_j\)s which yield more reasonable predictions for other values of \(\vec{x}\) than those contained in the training data set \(D\)\cite{mehta}.

Using the derivative of the first term from~\vref{eq:olsgrad}, the gradient of the cost function is
\begin{equation}
    \nabla Q_\lambda = -2 X^T \qty(\vec{y}-X\vec{\beta}) + 2\lambda \vec{\beta}
                     = -2 X^T \vec{y} + 2 X^T X \vec{\beta} + 2\lambda \vec{\beta}
                     = -2 X^T \vec{y} + 2\qty(X^T X + \lambda I)\vec{\beta}.
\end{equation}
Minimisation requires \(\nabla Q_\lambda = \vec{0}\), which gives
\begin{equation}
    \qty(X^T X + \lambda I)\vec{\beta} = X^T \vec{y}.
\end{equation}
This can be solved for \(\vec{\beta}\), and the closed-form solution to the minimisation of the Ridge cost function is
\begin{equation}
    \vec{\beta}_\text{Ridge} = \qty(X^T X + \lambda I)^{-1} X^T \vec{y}.
\end{equation}
Having a non-zero \(\lambda\) clearly reduces problems with linearly dependent columns of \(X\) and singularity of \(X^T X\).

Ridge regression has been implemented by calculating \(X^T X\) and adding \(\lambda\) on the diagonal, calculating \(X^T y\) and using \lstinline{dposv} to find \(\vec{\beta}\) using a Cholesky-decomposition, since \(X^T X + \lambda I\) is positive definite.

% _
%| | __ _ ___ ___  ___
%| |/ _` / __/ __|/ _ \
%| | (_| \__ \__ \ (_) |
%|_|\__,_|___/___/\___/
\subsection{LASSO regression}
The cost function used in LASSO regression is
\begin{equation}
    Q_\lambda(\vec{\beta};D) = \norm{\vec{y}-X\vec{\beta}}_2^2 + \lambda \norm{\vec{\beta}}_1
                             = \sum_{i=1}^N \qty(y_i - X_{ij}\beta_j)^2 + \lambda\sum_{i=1}^p \abs{\beta_i},
\end{equation}
which also penalises solution vectors \(\vec{\beta}\) where some coefficients are large.
LASSO regression has the advantage that certain choices of \(\lambda\) will give a sparse solution, i.e. \(\beta_j=0\) for some values of \(j\)\cite{wieringen}.

As with the other regression methods, the gradient of the cost function should now be differentiated and set to zero.
Mathematicians will now point out that the absolute value is not differentiable at zero and start deriving ingenious minimisation methods to circumvent the problem.
This is usually a non-issue in a physics course --- I will now \emph{define} the derivative to be
\begin{equation}
    \dv{\abs{x}}{x} \coloneqq \sgn{x} \coloneqq \begin{cases}1, & x > 0\\ 0, & x = 0 \\ -1, & x < 0 \end{cases}
\end{equation}
and proceed happily.

The gradient of the cost function is thus
\begin{equation}
    \nabla Q_\lambda = -2 X^T \qty(\vec{y} - X\vec{\beta}) + \lambda \sgn\qty(\vec{\beta}),
\end{equation}
and \(\nabla Q_\lambda = \vec{0}\) does unfortunately not have a closed-form solution.
A separate minimisation algorithm must therefore be used to find the parameters \(\beta_j\) which minimise the cost function.
The calculated gradient can be put straight into the gradient descent algorithm with a suitable step length.
Alternatively, Newton's method can be used with the second derivative (Hessian) matrix of the cost function.
The second derivative of the absolute value is even more problematic, which is solved by setting it equal to zero.

The Hessian matrix of the LASSO cost function is thus the same as the second derivative of the ordinary least squares cost function,
\begin{equation}
    H_{kl} = H_{lk} = \pdv{Q_\lambda}{\beta_l}{\beta_k}
           = \pdv{\beta_l}(\nabla_k Q)
           \,\husk{gradref}{=}\, \pdv{\beta_l}(-2X_{ik}\qty(y_i-X_{ij}\beta_j))
           = 2X_{ik}X_{il},
\tikz[>=latex,overlay,remember picture,thick]{\draw[<-,gronn] (gradref) .. controls ++(0,0.75) and ++(-1,0) .. ++(1,0.75) node[anchor=west] {\small\ref{eq:olsgradcomp}};}
\end{equation}
which is the component form of the matrix equation
\begin{equation}
    H = 2 X^T X.
\end{equation}
The gradient can now be rewritten as
\begin{equation}
    \nabla Q_\lambda = -2 X^T \vec{y} + H\vec{\beta} + \lambda \sgn\qty(\vec{\beta}).
\end{equation}
\Vref*{eq:newtonstep} in Newton's method can then transformed into
\begin{equation}
    H\qty(\vec{\beta}_{i+1} - \vec{\beta}_i) = 2X^T \vec{y}  - H\vec{\beta}_i - \lambda \sgn\qty(\vec{\beta}_i)
    \implies H\vec{\beta}_{i+1} = 2X^T\vec{y} - \lambda \sgn\qty(\vec{\beta}_i). \label{eq:lassonewton}
\end{equation}
Since \(H=2X^T X\), this reduces to the normal equations of ordinary least squares (\vref*{eq:olsnormal}) in the case of \(\lambda=0\), as it should.

Minimisation of the LASSO cost function has the benefit that the second derivative is independent of \(\vec{\beta}\).
\(H\) can therefore be Cholesky-decomposed once, an \(\mathcal{O}\qty(p^3)\) operation, and the result can be used to solve the linear set of equations for each iteration, which is \(\mathcal{O}\qty(p^2)\) with a pre-Cholesky-decomposed matrix.
The Cholesky-decomposition can be used instead of an LU-decomposition, reducing the number of floating point operations by a factor of two, because \(H=2X^T X\) is positive definite when \(X\) is non-singular.

Evaluation of the gradient only involves matrix-vector products, which is also an \(\mathcal{O}\qty(p^2)\) operation.
The minimisation can, therefore, be done with one initial \(\mathcal{O}\qty(p^3)\) and then only \(\mathcal{O}\qty(p^2)\) operations per iteration, while the more general problem requires both the construction and the Cholesky-decomposition of \(H\), an \(\mathcal{O}\qty(p^3)\) operation, for every single iteration.

This implementation gives both good performance and accurate results for small values of \(\lambda\).
Larger values, however, give rise to issues.
The theory suggests that a large \(\lambda\) should force some of the parameters \(\beta_j\) to become zero, but the cost function is not differentiable, and certainly not double differentiable, when \(\beta_j\) is zero.
Consequently, Newton's method struggles to converge when some \(\beta_j\)s are zero in the true minimum of the LASSO cost function.
Smarter methods such as coordinate descent\cite{friedman} should therefore be used, as in e.g.\ scikit-learn.


%           _       _           _           _   _
% _ __ ___ (_)_ __ (_)_ __ ___ (_)___  __ _| |_(_) ___  _ __
%| '_ ` _ \| | '_ \| | '_ ` _ \| / __|/ _` | __| |/ _ \| '_ \
%| | | | | | | | | | | | | | | | \__ \ (_| | |_| | (_) | | | |
%|_| |_| |_|_|_| |_|_|_| |_| |_|_|___/\__,_|\__|_|\___/|_| |_|
\subsection{Minimisation methods}\label{subsec:min}
The important step in a regression method, or indeed in any statistical learning method, is to minimise the cost function.
Certain cost functions, such as those of ordinary least squares and Ridge regression, admit analytical closed-form solutions for the parameters \(\beta_j\) which minimise \(Q(\vec{\beta};D)\), while most methods, including LASSO regression and logistic regression, require usage of some general minimisation algorithm.

\subsubsection{Newton's method}\label{subsubsec:newton}
Since the gradient of the cost function, \(\nabla Q\), is a perfectly normal function from \(\mathbb{R}^p\) to \(\mathbb{R}^p\), it can be Taylor expanded around some point \(\vec{\beta}_0\).
This Taylor expansion can then be evaluated at a point \(\vec{\beta}_0 + \delta\vec{\beta}\). To first order in \(\delta\vec{\beta}\),
\begin{equation}
    \nabla Q\qty(\vec{\beta}_0 + \delta\vec{\beta}) = \nabla Q\qty(\vec{\beta}_0) + H\qty(\vec{\beta}_0)\delta\vec{\beta},
\end{equation}
where \(H(\vec{\beta}_0)\) is the derivative of \(\nabla Q\), i.e.\ the Hessian of \(Q\), evaluated at \(\beta_0\).
Given a guess \(\beta_0\) somewhere near the minimum of \(Q\), a better estimate can be found by solving for the \(\delta\vec{\beta}\) which makes \(\nabla Q(\vec{\beta}_0 + \delta\vec{\beta}) = \vec{0}\), i.e.
\begin{equation}
    H\qty(\vec{\beta}_0)\delta\vec{\beta} = -\nabla Q\qty(\vec{\beta}_0),
\end{equation}
and, in general, solving
\begin{equation}
    H\qty(\vec{\beta}_i)\delta\vec{\beta}_i = -\nabla Q\qty(\vec{\beta}_i),\label{eq:newtonstep}
\end{equation}
letting \(\vec{\beta}_{i+1} = \vec{\beta}_i + \delta\vec{\beta}_i\) and continuing the process until the method has converged sufficiently.
The above equation is a simple linear set of equations.


\subsubsection{Gradient descent}
Evaluating the Hessian matrix and inverting it can sometimes be infeasible, for example due to poor time complexity or being woefully undefined.
In such cases, one can replace the Hessian \(H\) with a number \(1/\alpha\).
\Vref{eq:newtonstep} is then rewritten as
\begin{equation}
    \vec{\beta}_{i+1} = \vec{\beta_i} - \alpha \nabla Q\qty(\vec{\beta}_i),
\end{equation}
with a simple geometric interpretation:
By going a small step in the opposite direction of the gradient, the value of the cost function will decrease and \(\vec{\beta}\) will approach the true minimum.
A small step will guarantee convergence for a convex function at the cost of slow convergence, while a larger value may give faster convergence or not converge at all.

%                   __
%  _ __   ___ _ __ / _| ___  _ __ _ __ ___   __ _ _ __   ___ ___
% | '_ \ / _ \ '__| |_ / _ \| '__| '_ ` _ \ / _` | '_ \ / __/ _ \
% | |_) |  __/ |  |  _| (_) | |  | | | | | | (_| | | | | (_|  __/
% | .__/ \___|_|  |_|  \___/|_|  |_| |_| |_|\__,_|_| |_|\___\___|
% |_|
\subsection{Performance of regression methods}
While the cost function is minimised by all regression methods, its value is not particularly meaningful.
In particular, the cost functions in Ridge and LASSO regression depend on the parameter \(\lambda\), and a measure independent of \(\lambda\) should be used to determine the optimal \(\lambda\).
Other functions are therefore introduced to measure the performance of a given regression method and/or a choice of parameters.

The simplest measure is the mean square error,
\begin{equation}
    \MSE\qty(\vec{\beta},D) = \frac{1}{N}\norm{\vec{y}-\vec{\tilde{y}}}_2^2 = \frac{1}{N} \sum_{i=1}^N \qty(y_i - \tilde{y}_i)^2,
\end{equation}
which should be as small as possible. Another measure is the \(R^2\) score, defined as
\begin{equation}
    R^2(\vec{\beta},D) = 1 - \frac{\norm{\vec{y} - \vec{\tilde{y}}}_2^2}{\norm{\vec{y} - \bar{y}}_2^2}
                       = 1 - \frac{\sum_{i=1}^N \qty(y_i - \tilde{y}_i)^2}{\sum_{i=1}^N \qty(y_i - \bar{y})^2},
\end{equation}
where \(\bar{y}\) is the mean of the measured values. The \(R^2\) score should be as close to \(1\) as possible.

Prediction and these measures of performance can be applied to two main types of data.
Firstly, it can be applied to the data from which \(\vec{\beta}\) was derived, which is called the training data.
Ordinary least squares, corresponding to \(\lambda=0\) for the other methods, will, by definition, give the best results for this data set.
Secondly, the performance can be measured for values not among the training data, called test data. Ridge and LASSO are expected to outperform ordinary least squares for small, non-zero values of \(\lambda\) for this category of data.

%                                      _ _
%  _ __ ___  ___  __ _ _ __ ___  _ __ | (_)_ __   __ _
% | '__/ _ \/ __|/ _` | '_ ` _ \| '_ \| | | '_ \ / _` |
% | | |  __/\__ \ (_| | | | | | | |_) | | | | | | (_| |
% |_|  \___||___/\__,_|_| |_| |_| .__/|_|_|_| |_|\__, |
%                               |_|              |___/
\subsection{Resampling methods}
Resampling methods are techniques to improve the prediction accuracy and obtain estimates for quantities such as the variance of \(\vec{\beta}\) by repeatedly dividing the data set into training and test data.
Two simple examples are \(k\)-fold cross-validation and bootstrapping.
The former partitions the data set into \(k\) partitions.
One of these subsets is chosen as test data on which the performance is measured, while the rest are used as training data.
This process is the repeated \(k\) times, so that each subset is used once as test data.
Bootstrapping, on the other hand, repeatedly creates training data sets by randomly selecting \(N\) values from the data set with replacement, while another, separate data set is used as test data each time, as illustrated by the snippet below.
\lstinputlisting[linerange={bootstrapstart-bootstrapend}]{regression/src/bootstrap.f90}

%  _     _                             _
% | |__ (_) __ _ ___    __ _ _ __   __| |
% | '_ \| |/ _` / __|  / _` | '_ \ / _` |
% | |_) | | (_| \__ \ | (_| | | | | (_| |
% |_.__/|_|\__,_|___/  \__,_|_| |_|\__,_|
% __   ____ _ _ __(_) __ _ _ __   ___ ___
% \ \ / / _` | '__| |/ _` | '_ \ / __/ _ \
%  \ V / (_| | |  | | (_| | | | | (_|  __/
%   \_/ \__,_|_|  |_|\__,_|_| |_|\___\___|
\subsection{Bias and variance}
The choice of the number of basis functions, \(p\), determines the complexity of the model to which the data set is fitted.
A higher complexity makes the model a better approximation to the training data, which is said to reduce the \emph{bias} of the model.
On the other hand, a higher complexity may decrease the model's ability to predict reasonable values for test data, since a more complex model will be more affected by noise in the model.
This is said to increase the model's \emph{variance}.
The best prediction performance is therefore achieved for a complexity which balances bias and variance, which is called the bias-variance trade-off\cite{mehta}.

Following~\cite{mehta}, a mathematical manifestation of the bias-variance trade-off can be derived by assuming that the measured data set \(D=\qty{\qty(\vec{x}_i,y_i)}_{i=1}^N\) is generated by a combination of some model \(f\qty(\vec{x})\) (for example Franke's function) and random noise, specifically
\begin{equation}
    y_i = f\qty(\vec{x}_i) + \varepsilon_i \eqqcolon f_i + \varepsilon_i,
\end{equation}
where the variables \(\varepsilon_i\) are independent and normally distributed with variance \(\sigma^2\) around zero.
Given a data set \(D\), a prediction \(\tilde{y}_i^D\) can be made by any of the regression methods discussed above.
An expected mean square error can be found by averaging over all datasets \(D\) and all noises \(\vec{\varepsilon}\),
\begin{alignat}{2}
    E_{D,\varepsilon}\qty[\norm{\vec{y}-\vec{\tilde{y}}_D}_2^2]
    &= E_{D,\varepsilon}\qty[\norm{\vec{y} - \vec{f} + \vec{f} - \vec{\tilde{y}}_D}_2^2] \label{eq:tmp}
     = E_{D,\varepsilon}\qty[\norm{\vec{y} - \vec{f}}_2^2 + \norm{\vec{f} - \vec{\tilde{y}}_D}_2^2
                             + 2 \qty(\vec{y} - \vec{f})\cdot\qty(\vec{f} - \vec{\tilde{y}}_D)],
    \shortintertext{and using the linearity of the expectation value,}
    &= E_{D,\varepsilon}\qty[\norm{\vec{y} - \vec{f}}_2^2] + E_{D,\varepsilon}\qty[\norm{\vec{f} - \vec{\tilde{y}}_D}_2^2]
                             + 2 E_{D,\varepsilon}\qty[\qty(\vec{y} - \vec{f})\cdot\qty(\vec{f} - \vec{\tilde{y}}_D)]\\
    &= E_{D,\varepsilon}\qty[\norm{\vec{\varepsilon}}_2^2] + E_{D,\varepsilon}\qty[\norm{\vec{f} - \vec{\tilde{y}}_D}_2^2]
       + 2 E_{D,\varepsilon}\qty[\vec{\varepsilon}\cdot\qty(\vec{f} - \vec{\tilde{y}}_D)]\\
    &= \sigma^2 + E_{D,\varepsilon}\qty[\norm{\vec{f} - \vec{\tilde{y}}_D}_2^2]
       + 2 \cancel{E_{\varepsilon}\qty[\vec{\varepsilon}]}\cdot E_{D}\qty[\qty(\vec{f} - \vec{\tilde{y}}_D)]\\
    &= \sigma^2 + E_{D}\qty[\norm{\vec{f} - \vec{\tilde{y}}_D}_2^2].
\end{alignat}
This expression can be further decomposed by adding and subtracting the average prediction, \(E_D\qty[\vec{\tilde{y}}_D]\),
\begin{alignat}{2}
    E_{D}\qty[\norm{\vec{y}-\vec{\tilde{y}}_D}_2^2]
    &= \sigma^2 + E_{D}\qty[\norm{\vec{f}- E_D\qty[\vec{\tilde{y}}_D] + E_D\qty[\vec{\tilde{y}}_D] - \vec{\tilde{y}}_D}_2^2]\\
    &= \sigma^2 + \norm{\vec{f}- E_D\qty[\vec{\tilde{y}}_D]}_2^2 + E_D\qty[\norm{E_D\qty[\vec{\tilde{y}}_D] - \vec{\tilde{y}}_D}_2^2]\\
    &\phantom{{}={}} + 2E_D\qty[\qty(\vec{f}- E_D\qty[\vec{\tilde{y}}_D])\cdot\cancel{\qty(E_D\qty[\vec{\tilde{y}}_D] - \vec{\tilde{y}}_D)}]\\
    &= \sigma^2 + \norm{\vec{f}- E_D\qty[\vec{\tilde{y}}_D]}_2^2 + E_D\qty[\norm{E_D\qty[\vec{\tilde{y}}_D] - \vec{\tilde{y}}_D}_2^2].
\end{alignat}
The first term, \(\sigma^2\), represents the noise in the generated data, which no model can overcome.
The second term measures the deviation of the average prediction from the true, noise-free model, which is the (squared) bias.
Lastly, the third term measures how much the predictions vary and is called the variance.
A complex model (large \(p\)) will minimise the second term, since a complex model will be better suited to fit the true model, \(f\), while the third term will increase with model complexity since the fitting procedure will be more sensitive to noise in the different data sets.

Unfortunately, the above bias-variance decomposition requires knowledge of the exact model behind the data, \(f\).
A more computationally practical expression could have been derived by adding and subtracting \(E_D\qty[\vec{\tilde{y}}_D]\) in~\vref{eq:tmp}, which gives the expression
\begin{equation}
    E_{D}\qty[\norm{\vec{y}-\vec{\tilde{y}}_D}_2^2]
    = \norm{\vec{y}- E_D\qty[\vec{\tilde{y}}_D]}_2^2 + E_D\qty[\norm{E_D\qty[\vec{\tilde{y}}_D] - \vec{\tilde{y}}_D}_2^2]
\end{equation}
by manipulations analogous to the ones used above. This is another formulation of the bias-variance decomposition which can be used without any information about the underlying model. Dividing by \(N\) gives the average \(\MSE\) on the left-hand side.


\end{document}
